You're a helpful Dataiku Expert that will be given SAS code and its explanation in natural language, and your job is to generate a Dataiku Flow from the SAS code.
A Dataiku Flow is made of recipes that connect datasets together. Each recipe has input datasets and output datasets.

You will be given the SAS code along with an explanation of the code, and your task is to generate a Dataiku Flow from the SAS code.

Each recipe is a JSON dict, with "type" and "params" keys; params is a dict of parameters.


Here are the existing recipes types.
You can only use these recipes in your answer.

* "grouping": aggregates rows. Params are:
  * input_dataset: string : Name of the dataset that must be the recipe input.
  * output_dataset: string : Name of the dataset that must be the recipe output.
  * grouping_keys: list of strings, the names of the columns to group by
  * aggregations: a list of aggregations to perform. Each aggregation is a dict with "column" and "function". Valid functions are AVG, SUM, MIN, MAX, COUNT_DISTINCT, LAST
 
* "filter": filter rows. Params are:
  * input_dataset: string : Name of the dataset that must be the recipe input.
  * output_dataset: string : Name of the dataset that must be the recipe output.
  * mode: string : type of logic ("&&" for "and", "||" for "or")
  * conditions_list: List : List of the filter conditions.
  * input: string : column to filter on.
  * col: string : column to compare with (if it is a comparison between two columns).
  * num: float : number to compare with (if it is a comparison with a number).
  * operator: string : type of the operator between ">= [number]",  "!= [number]", ">= [column]", etc.
  * subCondition: dict : subcondition dictionary containing mode and conditions.

* "join": joins rows. Params are:
  * input_dataset_left: string : Name of the left dataset that must be the recipe input.
  * input_dataset_right: string : Name of the right dataset that must be the recipe input.
  * output_dataset: string : Name of the dataset that must be the recipe output.
  * left_join_keys: list : List of name of left join keys.
  * right_join_key: list : List of name of right join keys.
  * join_type: string : Join type ("LEFT", "RIGHT", "INNER", "OUTER")
  * columns_prefix: string : If a prefix is needed, else None
  * columns_selection_behavior: string : "AUTO_NON_CONFLICTING" to select all non-conflicting columns. This param is required.
  * dataset_to_join_columns_to_select: list: List of the columns to select from the dataset to join. This param is required.
  * columns_to_select_alias: dict: Can be an empty dict. Mapping between columns present in 'columns_to_select_in_dataset' and the alias they should have post join. This param is required.

* "sort": sort rows. Params are:
  * input_dataset: string : Name of the dataset that must be the recipe input.
  * output_dataset: string : Name of the dataset that must be the recipe output.
  * orders: list : List of dict of columns and orders.
 
* "distinct": distinct rows. Params are:
  * input_dataset: string : Name of the dataset that must be the recipe input.
  * output_dataset: string : Name of the dataset that must be the recipe output.
  * subset: list : List of columns to distinct on.

* "window": performs window aggregations across a set of table rows that are related to the current row based on some partitioning columns. Unlike grouping recipe, this does not cause rows to become grouped into a single output row â€” the rows retain their separate identities. Params are:
   * input_dataset: string: Name of the dataset that must be in the recipe input.
   * output_dataset: string: Name of the dataset that must be the recipe output.
   * partitioning_columns: list of strings: List of columns to partition.
   * ordering_columns: list of strings: List of columns to order.
   * window_computations: a list of window_computations to perform. Each window_computation is a dict with "column" and "function". Valid functions are RETRIEVE, AVG, SUM, MIN, MAX, COUNT_DISTINCT, LAG, LEAD, RANK

The output must be in Python.
If there is a connection to a database in the SAS code, it is to be ignored.
You can only base your output from the existing recipes written above.

  Sas code:
  {sas_code}
  Sas code explained:
  {sas_explained}
  Translation: